# This class file provides the fundamental code functionality for the 
# implementation of the Encoder component which lies at the core of the 
# Transformer Model. In particular, this functional component realizes the 
# Encoder Layer which consists of one Multi-Head Attention layer, one 
# Position-Wise Feed-Forward layer, and two Layer Normalization layers.

# The EncoderLayer class initializes with the required input parameters and the 
# following structural components:
# [i]:   One Multi Head Attention module for un-masked self-attention.
# [ii]:  One Position-Wise Feed Forward Neural Network module.
# [iii]: Two Layer Normalization modules.
# [iv]:  A Dropout Layer.

# The forward method computes the encoder layer output by applying 
# self-attention, adding the attention output to the tensor corresponding to 
# each input instance, and normalizing the result. Subsequently, the position-
# wise feed-forward output is computed which is, in turn, combined with the 
# normalized self-attention output. The final result provided by this module
# is normalized before returning the processed tensor.

# The required input parameters for the class constructor are the following:
# [i]:   The dimensionality d_model of each instance of the input sequence.
# [ii]:  The number num_heads of the attention heads realized by this 
#        component.
# [iii]: The intrinsic dimensionality d_ff of the combined linear 
#        transformation layer which is realized by this component.
# [iv]:  The percentage of connections that will be eliminated when information
#        flows through each dropout layer.

# Import required libraries.
import torch.nn as nn
from classes.MultiHeadAttention import MultiHeadAttention
from classes.PositionWiseFeedForward import PositionWiseFeedForward

class EncoderLayer(nn.Module):
    
    # Class Constructor.
    def __init__(self, d_model, num_heads, d_ff, dropout):
        # Call the super class constructor.
        super(EncoderLayer, self).__init__()
        # Add the Self Attention structural component of the Encoder.
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        # Add the Point-Wise Feed Forward Neural Network component of the 
        # Encoder.
        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)
        # Add the first Normalization Layer component of the Encoder.
        self.norm1 = nn.LayerNorm(d_model)
        # Add the second Normalization Layer component of the Encoder.
        self.norm2 = nn.LayerNorm(d_model)
        # Add the Dropout Layer component of the Encoder.
        self.dropout = nn.Dropout(dropout)
        
    # Forward Pass Function.
    def forward(self, x, mask):
        # Take into consideration that the current input instance is repeated
        # through the input arguments Q, K and V of the multi-head attention
        # layer.
        attn_output = self.self_attn(x, x, x, mask)
        # The first normalization layer performs dropout on the multi-head 
        # attention output and the final normalization output is generated
        # by considering a residual link which is directly connected to the 
        # currently presented input instance.
        x = self.norm1(x + self.dropout(attn_output))
        # Compute the output of the point-wise feed forward neural network 
        # layer.
        ff_output = self.feed_forward(x)
        # The second normalization layer performs dropout on the feed-forward
        # network output and the final normalization output is once again 
        # generated by considering a residual link which is directly connected 
        # to the currently presented input instance.
        x = self.norm2(x + self.dropout(ff_output))
        return x
        