## Continue LanguageModel.py and RecurrentNeuralNetworkModel.py


## Long Short Term Memory Networks [LSTMs]

Lecture **2019_6S191_L2.pdf** page 69.


### LSTM Cell
Cell State $c(t)$ is maintained in order to facilitate the information flow within the cell.

Output State/Hidden State $(h_r)$ is utilized in order to outpu parts of the cell state.

LSTMs operation is based on 3 dundamental processes:
1. forget
    forget Irrelevant parts of the previous state
3. update
    update cell state to reflect information according to the new input
5. output
    outputs certain parts of the cell state

All these 3 operations are implemented through the utilization of 3 logical gates.


### Forget Process
#### Forget Irrelevant Information


$$
f(t) = σ(W_i \cdot [h_{t-1}, x_t] + b_f)
$$

σ: Sigmoid value is in the [0, 1] range
$f_t \in [0,1]$: Βαθμός Λήθης: 0 => don't forget, 1=> forget everything

For example we might want to forget the gender pronoun of the previous subject of the sentence.


### Update Process
#### Identify New Information to be Stored. 

$$
i_t = σ(W_i \cdot [h_{t-1}, x_t] + b_i)
$$
$$
\tilde {C_t} = tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
$$
Sigmoid Layer decides which values to update
Tanh Layer generates a new vector of "candidate values" that could be added to the cell state.

For example we might want to add the gender of the new subject.

#### Update the Cell State
$$
C_t = f_t * C_{t-1} + i_t * \tilde {C_t}
$$
For example drop all previous  information and add new inforamtion conerning the gender of the new subject

$C_{t-1}$: Backward Inforamtion Pass.
$C_t$: Forward Inforamtion Pass.

During the backward information pass only element wise multiplications are performed. Thuss the vanishing gradients problem is aleviated. Κατά μια έννοια "παγιδεύει το Gradient Information μέσα στο LSTM κύτταρο".

### Output Process
#### Output a filtered version fo the previous cell state
$$
O_t = σ(W_o \cdot [h_t-1, x_t] + b_o)
$$
$$
h_t = O_t * tanh(C_t)
$$
Sigmoid Layer: decide what parts of the cell state will be sent to the output.
Tanh Layer: Squeezes information in $[-1, +1]$.
$O_t$: Final Output

For exmaple, having seen the new subject, we might output information concerning the new verb.