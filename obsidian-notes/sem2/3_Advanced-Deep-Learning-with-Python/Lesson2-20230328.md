## The problem of Vanishing Gradients (εξαφανιζόμενων διανυσμάτων βαθμίδας) in vanila RNNs (Simple Language Model using RNNs)


Gradient of the loss function (with respect to $W_{xy}$)

$$


\frac{\partial J_t}{\partial W_{xh}}

= \sum_{k=0}^t
\frac{\partial J_t}{\partial W_{\hat y_t}}
\cdot \frac{\partial \hat y_t}{\partial h_t}
\cdot ...

$$

We ned to quanitify the dependents of the form:
$$
\frac {\partial h_t}{\partial h_{t-1}}
$$
Let our RNN be a single neuron RNN such that:
1. 
$$
x_t \in \Re ^ {l \times 1}, 1 \le t \le z
$$
... and other conditions

Activation Function:
$$
f(u) = \tanh (u) = \frac {e^u - e^{-u}}{e^u + e^{-u}}
$$
Υπερβολική Εφαπτομένη (Sigmoid Activation Function)

μπορεί να αποδειχτεί:
$$
f'(u) = 1 -f^2(u)
$$
Let u be the one dimensional input provided in the activation function
$$
u = W^t_{xh} \cdot x_t + W_{hh} \cdot h_{t-1}
...
$$
Καταλήγουμε σε πολλαπλασιασμό μεγάλης ακολουθίας αριθμών < 1. Για αυτόν τον λόγο εξαφανίζονται οι εξαρτήσεςι από παλιά δεδομένα



